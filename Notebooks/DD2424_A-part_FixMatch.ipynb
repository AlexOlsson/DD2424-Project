{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679822149
        },
        "id": "c7YBTCQyGZC6"
      },
      "outputs": [],
      "source": [
        "%pip install comet_ml --quiet\n",
        "import comet_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jeifvMoObQC"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679835111
        },
        "id": "g-OR6-johB6i"
      },
      "outputs": [],
      "source": [
        "# Check version of packages\n",
        "import torch\n",
        "print(torch)\n",
        "\n",
        "import torchvision\n",
        "print(torchvision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679837789
        },
        "id": "X13Xl_SLhfoq"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egqoLkXXOfDz"
      },
      "source": [
        "Check Colab GPU and CPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679838273
        },
        "id": "dokutT2vN9VA"
      },
      "outputs": [],
      "source": [
        "# Check GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679838723
        },
        "id": "cl6KnThxOMDp"
      },
      "outputs": [],
      "source": [
        "# Check CPU info\n",
        "!lscpu |grep 'Model name'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbmMWruZkkw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOgPs_RObYX4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if not os.path.exists(\"images.tar.gz\"):\n",
        "  # Download dataset\n",
        "  !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "\n",
        "  # Download truth label\n",
        "  !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
        "  # Unpack the dataset and ground truth\n",
        "  filename_images = \"images.tar.gz\"\n",
        "  !tar -zxvf {filename_images}\n",
        "\n",
        "  filename_annotations = \"annotations.tar.gz\"\n",
        "  !tar -zxvf {filename_annotations}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge-66OXBwdsg"
      },
      "source": [
        "## CATEGORIZE DATA AFTER ANIMAL BREED (37 IN TOTAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679612
        },
        "id": "QMyrI7FIwhVW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "import random\n",
        "SEED_FOR_DATA_SPLIT = 13948571\n",
        "temp_state_file = \"temp_state_file\"\n",
        "do_img_move = True\n",
        "\n",
        "if os.path.exists(temp_state_file):\n",
        "  with open(temp_state_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      if \"FILES_ALREADY_MOVED\" in line:\n",
        "        do_img_move = False\n",
        "        break\n",
        "\n",
        "if do_img_move:\n",
        "  # 37 classes\n",
        "  annotations_file = \"annotations/list.txt\"\n",
        "  images_base_path = \"images/\"\n",
        "  res_base_dir = \"sorted_imgs\"\n",
        "  classes = defaultdict(list)\n",
        "  with open(annotations_file) as f:\n",
        "      for line in f:\n",
        "          if line.startswith(\"#\"):\n",
        "              continue\n",
        "          name, class_id, species, _ = line.strip().split(\" \")\n",
        "          classes[int(class_id)].append(name + \".jpg\")\n",
        "\n",
        "  for class_id, files in classes.items():\n",
        "      os.makedirs(f\"{res_base_dir}/{str(class_id)}/\", exist_ok=True)\n",
        "      \n",
        "      number_of_files = len(files)\n",
        "      for i in range(number_of_files):\n",
        "          shutil.move(os.path.join(images_base_path, files[i]), f\"{res_base_dir}/{str(class_id)}/{files[i]}\")\n",
        "\n",
        "  with open(temp_state_file, \"a\") as f:\n",
        "    f.write(\"FILES_ALREADY_MOVED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679662
        },
        "id": "qAgl15Oi3jZ5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initial inspiration: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets \n",
        "def dataset_split(dataset, ratio_to_first_split):\n",
        "  first_indices, second_indices, _, _ = train_test_split(\n",
        "        range(len(dataset)),\n",
        "        dataset.targets,\n",
        "        stratify=dataset.targets,\n",
        "        train_size=ratio_to_first_split,\n",
        "        random_state=SEED_FOR_DATA_SPLIT\n",
        "      )\n",
        "  \n",
        "  return first_indices, second_indices\n",
        "\n",
        "\n",
        "def subset_split(dataset, subset_indices, ratio_to_first_split):\n",
        "  target_labels = list(map(lambda i: dataset.targets[i], subset_indices))\n",
        "  \n",
        "  first_indices, second_indices, _, _ = train_test_split(\n",
        "        subset_indices,\n",
        "        target_labels,\n",
        "        stratify=target_labels,\n",
        "        train_size=ratio_to_first_split,\n",
        "        random_state=SEED_FOR_DATA_SPLIT\n",
        "      )\n",
        "  \n",
        "  return first_indices, second_indices\n",
        "\n",
        "\n",
        "def _get_datasets(ratio_in_labled_train_dataset, transforms_dict):\n",
        "  \"\"\"Return format: test_dataset, val_dataset, labled_train_dataset, unlabled_train_dataset \"\"\"\n",
        "  \n",
        "  labled_train_dataset = datasets.ImageFolder(\"sorted_imgs\", transforms_dict[\"labled\"])\n",
        "  unlabled_train_dataset = datasets.ImageFolder(\"sorted_imgs\", transforms_dict[\"unlabled\"])\n",
        "  test_and_val_dataset = datasets.ImageFolder(\"sorted_imgs\", transforms_dict[\"test_and_val\"])\n",
        "\n",
        "  train_and_val_indices, test_indices = dataset_split(labled_train_dataset, ratio_to_first_split=0.9)\n",
        "  train_indices, val_indices = subset_split(labled_train_dataset, train_and_val_indices, ratio_to_first_split=0.75)\n",
        "  \n",
        "  test_dataset = Subset(test_and_val_dataset, test_indices)\n",
        "  val_dataset = Subset(test_and_val_dataset, val_indices)\n",
        "\n",
        "  if ratio_in_labled_train_dataset == 1.0:\n",
        "    return test_dataset, val_dataset, Subset(labled_train_dataset, train_indices), None\n",
        "\n",
        "  train_labled_indices, train_unlabled_indices = subset_split(labled_train_dataset, train_indices, ratio_to_first_split=ratio_in_labled_train_dataset)\n",
        "  return test_dataset, val_dataset, Subset(labled_train_dataset, train_labled_indices), Subset(unlabled_train_dataset, train_unlabled_indices)\n",
        "\n",
        "\n",
        "def get_dataloaders_dict(ratio_in_labled_train_dataset, transforms_dict, batch_size):\n",
        "  \"\"\" Note: currently test set is fixed at 10 %, val at 20 % and rest (labled + unlabled) is 70 %\"\"\"\n",
        "\n",
        "  test_dataset, val_dataset, labled_train, unlabled_train = _get_datasets(ratio_in_labled_train_dataset, transforms_dict)\n",
        "  dataloaders_dict = {\n",
        "    \"test\": torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
        "    \"val\": torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
        "    \"labled\": torch.utils.data.DataLoader(labled_train, batch_size=batch_size, shuffle=True, num_workers=2),\n",
        "  }\n",
        "  # number of supervised and unsupervised batches will be the same. This will make looping through the dataloader easier\n",
        "  if unlabled_train:\n",
        "    nbr_supervised_batches = len(dataloaders_dict[\"labled\"].batch_sampler)\n",
        "    nbr_unsupervised_samples = len(unlabled_train.indices)\n",
        "    unsupervised_batch_size = round(nbr_unsupervised_samples / nbr_supervised_batches)\n",
        "  else:\n",
        "    unsupervised_batch_size=0\n",
        "  dataloaders_dict[\"unlabled\"] = None if not unlabled_train else torch.utils.data.DataLoader(unlabled_train, batch_size=unsupervised_batch_size, shuffle=True, num_workers=2)\n",
        "  \n",
        "  return dataloaders_dict\n",
        "\n",
        "\n",
        "input_size = 224\n",
        "data_transforms = {\n",
        "    'labled': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size,scale=(0.9,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'unlabled': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'strong_augment': transforms.Compose([\n",
        "            transforms.Lambda(lambda img: transforms.functional.solarize(img, threshold=256)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.autocontrast(img)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_brightness(img, brightness_factor=2)),\n",
        "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "            transforms.Lambda(lambda img: transforms.functional.equalize(img)),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.Lambda(lambda img: transforms.functional.posterize(img, bits=4)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_sharpness(img, sharpness_factor=2)),\n",
        "            #transforms.ToTensor(),\n",
        "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test_and_val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "dataloaders_dict = get_dataloaders_dict(ratio_in_labled_train_dataset=0.50, transforms_dict=data_transforms, batch_size=9)\n",
        "print(dataloaders_dict.keys())\n",
        "# NOTE: there is a new way to get number of batches:\n",
        "# Ex. len(dataloaders_dict[\"test\"].batch_sampler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDdLUR-e03CG"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def convert_to_PIL_image(tensor):\n",
        "    \"\"\"\n",
        "    Convert a tensor to PIL image(s)\n",
        "    :param tensor:\n",
        "    :return: List of PIL image(s)\n",
        "    \"\"\"\n",
        "    # define the mean and standard deviation values used for de-normalization (same as the ones used for normalization)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # assume \"tensor\" is your 4D normalized image tensor with shape (batch_size, channels, height, width)\n",
        "    batch_size = tensor.shape[0]\n",
        "    denormalized_tensor = torch.zeros_like(tensor)\n",
        "    for i in range(batch_size):\n",
        "        for channel in range(tensor.shape[1]):\n",
        "            denormalized_tensor[i][channel] = tensor[i][channel] * std[channel] + mean[channel]\n",
        "\n",
        "    # convert tensor(s) to a list of PIL images\n",
        "    denormalized_images = [transforms.ToPILImage()(denormalized_tensor[i]) for i in range(batch_size)]\n",
        "    return denormalized_images\n",
        "\n",
        "def strong_data_augmentation(unlabeld_data):\n",
        "    \"\"\"\n",
        "    Apply strong data augmentation techniques to unlabeled data for FixMatch\n",
        "    :param unlabeld_data: unlabeled image sample(s) stored in a normalized tensor\n",
        "    :return: augmented_data\n",
        "    \"\"\"\n",
        "\n",
        "    # Define data transformations\n",
        "    strong_data_transforms = {\n",
        "        'unlabled': transforms.Compose([\n",
        "            transforms.Lambda(lambda img: transforms.functional.solarize(img, threshold=256)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.autocontrast(img)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_brightness(img, brightness_factor=2)),\n",
        "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "            transforms.Lambda(lambda img: transforms.functional.equalize(img)),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.Lambda(lambda img: transforms.functional.posterize(img, bits=4)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_sharpness(img, sharpness_factor=2)),\n",
        "            #transforms.RandAugment(5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    # Convert tensors to PIL images\n",
        "    unlabled_input_weak_images = convert_to_PIL_image(unlabeld_data.cpu())\n",
        "\n",
        "    # Apply strong data augmentation to unlabeled image samples\n",
        "    strong_augmented_images = []\n",
        "    for img in unlabled_input_weak_images:\n",
        "        img = strong_data_transforms['unlabled'](img)\n",
        "        strong_augmented_images.append(img)\n",
        "\n",
        "\n",
        "    # Collect tensors of augmented images in a single tensor, i.e. collect images in a single batch\n",
        "    strong_augmented_images = torch.stack(strong_augmented_images)\n",
        "\n",
        "    for image in convert_to_PIL_image(strong_augmented_images):\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    # Display strongly augmented images\n",
        "    return strong_augmented_images\n",
        "\n",
        "unlabeled_data2 = Image.open(\"sorted_imgs/1/Abyssinian_1.jpg\")\n",
        "for x,y in dataloaders_dict[\"unlabled\"]:\n",
        "  #print(x.shape)\n",
        "  #print(convert_to_PIL_image(x))\n",
        "\n",
        "  imgs = strong_data_augmentation(x)\n",
        "  #print(imgs)\n",
        "  #print(imgs)\n",
        "  # Display the image\n",
        "  #plt.imshow(unlabeled_data2)\n",
        "  #plt.imshow(imgs)\n",
        "  #plt.axis('off')\n",
        "  #plt.show()\n",
        "\n",
        "  #augmented_data2 = data_transforms[\"strong_augment\"](x[0])\n",
        "  # Display the augmented image\n",
        "  #plt.imshow(augmented_data2)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJoDoM_Tph6p"
      },
      "source": [
        "Download model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2aEPMod9w8"
      },
      "source": [
        "### Finetune resnet-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679734
        },
        "id": "t4c3JPULEWaD"
      },
      "outputs": [],
      "source": [
        "#from collections import Counter\n",
        "#target_labels = list(map(lambda i: dataloaders_dict[\"labled\"].dataset.dataset.targets[i], dataloaders_dict[\"labled\"].dataset.indices))\n",
        "#Counter(target_labels)\n",
        "#len(dataloaders_dict[\"unlabled\"].dataset.indices)\n",
        "#print(len(dataloaders_dict[\"labled\"].dataset), len(dataloaders_dict[\"unlabled\"].dataset))\n",
        "#print(len(dataloaders_dict[\"labled\"].batch_sampler), len(dataloaders_dict[\"unlabled\"].batch_sampler))\n",
        "#dataloaders_dict[\"labled\"].batch_size, dataloaders_dict[\"unlabled\"].batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679764
        },
        "id": "lfyORVvyecRJ"
      },
      "outputs": [],
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZa24RqahA7t"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679804
        },
        "id": "pXD6oA1Se33P"
      },
      "outputs": [],
      "source": [
        "def test_acc(model, dataloader_dict):\n",
        "  model.eval()\n",
        "  running_corrects = 0\n",
        "  for i, (inputs, labels) in enumerate(dataloader_dict[\"test\"]):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, pred_for_labled_data = torch.max(outputs, 1)                    \n",
        "    running_corrects += torch.sum(pred_for_labled_data == labels.data)\n",
        "  test_acc = running_corrects.double() / len(dataloader_dict[\"test\"].dataset)\n",
        "  \n",
        "  return test_acc.item()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679845
        },
        "id": "rlPRZkECs1jo"
      },
      "outputs": [],
      "source": [
        "def load_resnet_with_unfrozen_layers(num_unfrozen_layers, unfreeze_bn, num_classes=37):\n",
        "  model_ft = models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "  #model_ft = models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
        "  param_layers = []\n",
        "\n",
        "  for name, param in model_ft.named_parameters():\n",
        "    if not any(map(lambda layer_str: layer_str in name, [f\"layer{4-i}\" for i in range(num_unfrozen_layers)])) or (not unfreeze_bn and \"bn\" in name):\n",
        "      param.requires_grad = False\n",
        "\n",
        "  num_ftrs = model_ft.fc.in_features\n",
        "  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  return model_ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679679949
        },
        "id": "yBnIT1HYw5PB"
      },
      "outputs": [],
      "source": [
        "def aggressive_augment(unlabeled_data):\n",
        "    \"\"\"\n",
        "    Apply aggressive data augmentation techniques to unlabeled data\n",
        "    :param unlabeled_data:\n",
        "    :return: augmented_data\n",
        "    \"\"\"\n",
        "    input_size = 224\n",
        "\n",
        "    # Define data transforms\n",
        "    aggressive_data_transforms = {\n",
        "        'unlabeled': transforms.Compose([\n",
        "            transforms.Lambda(lambda img: transforms.functional.solarize(img, threshold=256)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.autocontrast(img)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_brightness(img, brightness_factor=2)),\n",
        "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "            transforms.Lambda(lambda img: transforms.functional.equalize(img)),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.Lambda(lambda img: transforms.functional.posterize(img, bits=4)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_sharpness(img, sharpness_factor=2)),\n",
        "            # transforms.ToTensor(),\n",
        "            # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    }\n",
        "    # Apply aggressive data augmentation to unlabeled data\n",
        "    augmented_data = aggressive_data_transforms['unlabeled'](unlabeled_data)\n",
        "    return augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679680025
        },
        "id": "_2K5isq7D_qL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def alpha_scheduler(current_epoch_num, T1, T2, max_alpha):\n",
        "  # PART 1: [ALPHA = 0]\n",
        "  if (current_epoch_num) < (T1): \n",
        "    return 0\n",
        "\n",
        "  # PART 2: epoch \"T1\" ---> epoch \"T2\"\n",
        "  elif (current_epoch_num) < (T2):\n",
        "    return (current_epoch_num-T1)/(T2-T1) * max_alpha\n",
        "  \n",
        "  # PART 3: [ALPHA = max_alpha]\n",
        "  return max_alpha\n",
        "\n",
        "\n",
        "def convert_to_PIL_image(tensor):\n",
        "    \"\"\"\n",
        "    Convert a tensor to PIL image(s)\n",
        "    :param tensor:\n",
        "    :return: List of PIL image(s)\n",
        "    \"\"\"\n",
        "    # define the mean and standard deviation values used for de-normalization (same as the ones used for normalization)\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # assume \"tensor\" is your 4D normalized image tensor with shape (batch_size, channels, height, width)\n",
        "    batch_size = tensor.shape[0]\n",
        "    denormalized_tensor = torch.zeros_like(tensor)\n",
        "    for i in range(batch_size):\n",
        "        for channel in range(tensor.shape[1]):\n",
        "            denormalized_tensor[i][channel] = tensor[i][channel] * std[channel] + mean[channel]\n",
        "\n",
        "    # convert tensor(s) to a list of PIL images\n",
        "    denormalized_images = [transforms.ToPILImage()(denormalized_tensor[i]) for i in range(batch_size)]\n",
        "    return denormalized_images\n",
        "\n",
        "def strong_data_augmentation(unlabeld_data):\n",
        "    \"\"\"\n",
        "    Apply strong data augmentation techniques to unlabeled data for FixMatch\n",
        "    :param unlabeld_data: unlabeled image sample(s) stored in a normalized tensor\n",
        "    :return: augmented_data\n",
        "    \"\"\"\n",
        "\n",
        "    # Define data transformations\n",
        "    strong_data_transforms = {\n",
        "        'unlabled': transforms.Compose([\n",
        "            transforms.Lambda(lambda img: transforms.functional.solarize(img, threshold=256)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.autocontrast(img)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_brightness(img, brightness_factor=2)),\n",
        "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
        "            transforms.Lambda(lambda img: transforms.functional.equalize(img)),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.Lambda(lambda img: transforms.functional.posterize(img, bits=4)),\n",
        "            transforms.Lambda(lambda img: transforms.functional.adjust_sharpness(img, sharpness_factor=2)),\n",
        "            #transforms.RandAugment(1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    # Convert tensors to PIL images\n",
        "    unlabled_input_weak_images = convert_to_PIL_image(unlabeld_data)\n",
        "\n",
        "    # Apply strong data augmentation to unlabeled image samples\n",
        "    strong_augmented_images = []\n",
        "    for img in unlabled_input_weak_images:\n",
        "        img = strong_data_transforms['unlabled'](img)\n",
        "        strong_augmented_images.append(img)\n",
        "\n",
        "    # Collect tensors of augmented images in a single tensor, i.e. collect images in a single batch\n",
        "    strong_augmented_images = torch.stack(strong_augmented_images)\n",
        "\n",
        "    return strong_augmented_images\n",
        "    \n",
        "\n",
        "def train_model_pseudo_labeling(model, dataloaders, criterion, optimizers, schedulers, experiment):\n",
        "    since = time.time()\n",
        "    num_epochs = experiment.get_parameter(\"num_epochs\")\n",
        "    labling_confidence = experiment.get_parameter(\"labling_confidence\")\n",
        "    val_acc_history = []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    alpha = experiment.get_parameter(\"fixed_alpha\")\n",
        "\n",
        "    step = -1\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        # training phase\n",
        "        phase = \"train\"\n",
        "        model.train()  # Set model to training mode\n",
        "        #alpha = alpha_scheduler(epoch, T1=experiment.get_parameter(\"T1\"), T2=experiment.get_parameter(\"T2\"), max_alpha=experiment.get_parameter(\"max_alpha\")) # Update alpha\n",
        "        #print('ALPHA =', alpha)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Create tqdm progress bar object\n",
        "\n",
        "        # Iterate over data.\n",
        "        \n",
        "        for (labled_input, labels), (unlabled_input_weak, hidden_labels) in zip(dataloaders[\"labled\"], dataloaders[\"unlabled\"]):\n",
        "            step += 1\n",
        "            labled_input = labled_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # FIXMATCH - PART 1: Apply aggressive data augmentation to unlabeled data\n",
        "            unlabled_input_weak_clone = unlabled_input_weak.clone().detach()\n",
        "            unlabled_input_weak = unlabled_input_weak.to(device)\n",
        "\n",
        "            # Generate unlabled images with strong data augmentation\n",
        "            unlabled_input_strong = strong_data_augmentation(unlabled_input_weak_clone)\n",
        "            unlabled_input_strong = unlabled_input_strong.to(device)\n",
        "            #unlabled_input_strong = unlabled_input_weak_clone.to(device)\n",
        "            hidden_labels = hidden_labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            for optimizer in optimizers:\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # supervised forward pass\n",
        "                supervised_forward_pass_outputs = model(labled_input)\n",
        "                _, pred_for_labled_data = torch.max(supervised_forward_pass_outputs, 1)\n",
        "                supervised_loss = criterion(supervised_forward_pass_outputs, labels)\n",
        "\n",
        "              \n",
        "                # predict pseudo labels for unlabeld data with weak data augmentation\n",
        "                unsupervised_out = model(unlabled_input_weak)\n",
        "                softmax_unsupervised_out = nn.functional.softmax(unsupervised_out, dim=1)\n",
        "                max_probs, pseudo_labels = torch.max(softmax_unsupervised_out, dim=1)\n",
        "                with torch.no_grad():\n",
        "                  confident_labels = (max_probs >= labling_confidence).sum().item()\n",
        "                  in_correct_label_count = (pseudo_labels[max_probs >= labling_confidence] != hidden_labels[max_probs >= labling_confidence]).sum().item()\n",
        "                  experiment.log_metric(\"confident pseudo labels\", confident_labels, step=step, epoch=epoch)\n",
        "                  experiment.log_metric(\"incorrect pseudo labels ratio\", in_correct_label_count / max(1, confident_labels), step=step, epoch=epoch)\n",
        "\n",
        "                # forward pass for unlabeled data with strong data augmentation\n",
        "                # forward pass for unlabeled data with strong data augmentation\n",
        "                strong_augm_unsupervised_out = model(unlabled_input_strong)\n",
        "                # calculate the loss for unlabeled data with strong data augmentation vs pseudo labels\n",
        "                # but only use pseudolables with high enough confidence\n",
        "                pseudo_label_loss = criterion(strong_augm_unsupervised_out[max_probs >= labling_confidence], pseudo_labels[max_probs >= labling_confidence])\n",
        "                if pseudo_label_loss.isnan().any():\n",
        "                  pseudo_label_loss = 0\n",
        "\n",
        "                # # combined loss\n",
        "                loss = supervised_loss + alpha * pseudo_label_loss\n",
        "                #loss = supervised_loss\n",
        "\n",
        "                # backward pass\n",
        "                loss.backward()\n",
        "                for optimizer in optimizers:\n",
        "                  optimizer.step()\n",
        "                for scheduler in schedulers:\n",
        "                  if type(scheduler) == lr_scheduler.OneCycleLR:\n",
        "                    scheduler.step()\n",
        "                labels = labels # Due to calc. of running_corrects\n",
        "            \n",
        "            #experiment.log_metric(\"scaled unsupervised loss\", alpha * pseudo_label_loss, step=step, epoch=epoch)\n",
        "            experiment.log_metric(\"unsupervised loss\", pseudo_label_loss, step=step, epoch=epoch)\n",
        "            experiment.log_metric(\"supervised loss\", supervised_loss, step=step, epoch=epoch)\n",
        "            experiment.log_metric(\"alpha\", alpha, step=step, epoch=epoch)\n",
        "            # statistics\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(pred_for_labled_data == labels.data)\n",
        "\n",
        "            # Update progress bar\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[\"labled\"].batch_sampler) # note len(dataloaders[phase].dataset) is nbr of samples not nbr of batches\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[\"labled\"].dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        experiment.log_metric(f\"train accuracy\", epoch_acc, step=step, epoch=epoch)\n",
        "\n",
        "        for scheduler in schedulers:\n",
        "          if scheduler is None or scheduler == lr_scheduler.OneCycleLR:\n",
        "            continue\n",
        "          if type(scheduler) == lr_scheduler.ReduceLROnPlateau:\n",
        "            scheduler.step(epoch_acc)\n",
        "          else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # validation phase starts here\n",
        "        phase = \"val\"\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Create tqdm progress bar object\n",
        "        print(f\"Processing {phase} data\")\n",
        "\n",
        "        # Iterate over data.\n",
        "        for (labled_input, labels) in dataloaders[\"val\"]:\n",
        "            labled_input = labled_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            for optimizer in optimizers:\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(False):\n",
        "                # Get model outputs and calculate loss\n",
        "                  supervised_forward_pass_outputs = model(labled_input)\n",
        "                  _, pred_for_labled_data = torch.max(supervised_forward_pass_outputs, 1)\n",
        "                  loss = criterion(supervised_forward_pass_outputs, labels)\n",
        "                    \n",
        "            # statistics\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(pred_for_labled_data == labels.data)\n",
        "\n",
        "            # Update progress bar\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[\"val\"].batch_sampler) # note len(dataloaders[phase].dataset) is nbr of samples not nbr of batches\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[\"val\"].dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        val_acc_history.append(epoch_acc)\n",
        "        experiment.log_metric(f\"val accuracy\", epoch_acc, step=step, epoch=epoch)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    experiment.log_metric(\"best model test acc\", test_acc(model, dataloaders_dict))\n",
        "    experiment.end()\n",
        "\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcaIAGdqk5oB"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cruuro4uJIo"
      },
      "source": [
        "Cell for experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4qWxREKlPqR"
      },
      "source": [
        "Run Training and Validation Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679680181
        },
        "id": "tZH1uwnalThf"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "  \"batch_size\": 9, \n",
        "  \"num_epochs\": 15,\n",
        "  \"fixed_alpha\": 1,\n",
        "  \"lr\": 0.001,\n",
        "  \"weight_decay\": 0.005,\n",
        "  \"ratio_of_labled_data\": 0.25,\n",
        "  \"labling_confidence\": 0.95,\n",
        "  \"unfrozen_layers\": 0,\n",
        "  \"seed\": SEED_FOR_DATA_SPLIT,\n",
        "}\n",
        "\n",
        "runs = [\n",
        "    (\"ratio_of_labled_data\", 0.10),\n",
        "    (\"ratio_of_labled_data\", 0.25),\n",
        "    (\"ratio_of_labled_data\", 0.9),\n",
        "    (\"ratio_of_labled_data\", 0.75),\n",
        "    (\"ratio_of_labled_data\", 0.02),\n",
        "    (\"ratio_of_labled_data\", 0.5),\n",
        "]\n",
        "\n",
        "for run in runs:\n",
        "  hyperparams_copy = hyperparams.copy()\n",
        "  hyperparams_copy[run[0]] = run[1]\n",
        "  experiment = comet_ml.Experiment(\n",
        "    api_key=\"aVOX2ucOyHDQwWZMY5hKCT5NJ\",\n",
        "    project_name=\"dd2424-project\",\n",
        "    workspace=\"storsorken\",\n",
        "  )\n",
        "  #for experiment in opt.get_experiments():\n",
        "  experiment.add_tag(\"FixMatchBatchRuns-rerun-again\")\n",
        "\n",
        "  experiment.log_parameters(hyperparams_copy)\n",
        "\n",
        "  model_ft = load_resnet_with_unfrozen_layers(experiment.get_parameter(\"unfrozen_layers\"), True, num_classes=37)\n",
        "\n",
        "  # creates a dict that groups model params into a dict with lists.\n",
        "  # all params for ex. layer4 can be accessed using layer_param_dict[\"layer4\"]\n",
        "\n",
        "  layer_param_dict = defaultdict(list)\n",
        "  for name,param in model_ft.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      layer_param_dict[name.split(\".\")[0]].append(param)\n",
        "\n",
        "  all_params = []\n",
        "  for key, param_list in layer_param_dict.items():\n",
        "    all_params += param_list\n",
        "\n",
        "  print(layer_param_dict.keys())\n",
        "\n",
        "  optimizers = [\n",
        "      optim.Adam(layer_param_dict[\"fc\"], lr=experiment.get_parameter(\"lr\"), weight_decay=experiment.get_parameter(\"weight_decay\")),\n",
        "      #optim.Adam(layer_param_dict[\"layer4\"], lr=0.001, weight_decay=1e-5),\n",
        "      #optim.Adam(layer_param_dict[\"layer3\"], lr=0.001),\n",
        "  ]\n",
        "  schedulers = [\n",
        "      lr_scheduler.ReduceLROnPlateau(optimizers[0], patience=2),\n",
        "      #lr_scheduler.LambdaLR(optimizers[1], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
        "      #lr_scheduler.LambdaLR(optimizers[2], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
        "  ]\n",
        "\n",
        "  dataloaders_dict = get_dataloaders_dict(experiment.get_parameter(\"ratio_of_labled_data\"), data_transforms, experiment.get_parameter(\"batch_size\"))\n",
        "\n",
        "  # Setup the loss fxn\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train and evaluate\n",
        "  model_ft, hist = train_model_pseudo_labeling(model_ft, dataloaders_dict, criterion, optimizers, schedulers, experiment)\n",
        "  # model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizers, schedulers, num_epochs=num_epochs)\n",
        "\n",
        "# plot results\n",
        "plt.plot(range(1, len(hist) + 1), list(map(lambda x: x.item(), hist)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Val accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679680222
        },
        "id": "6DLW1d2nhQtt"
      },
      "outputs": [],
      "source": [
        "# NOTE: run this line very sparinlgly\n",
        "#print(f\"Test acc: {test_acc(model_ft, dataloaders_dict)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1684679680255
        },
        "id": "8rKIWNKt63HY"
      },
      "outputs": [],
      "source": [
        "#len(dataloaders_dict[\"val\"].batch_sampler), (dataloaders_dict[\"val\"].batch_size), (dataloaders_dict[\"val\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
