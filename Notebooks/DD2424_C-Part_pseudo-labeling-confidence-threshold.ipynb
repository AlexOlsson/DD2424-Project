{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7YBTCQyGZC6"
      },
      "outputs": [],
      "source": [
        "%pip install comet_ml --quiet\n",
        "import comet_ml"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3jeifvMoObQC"
      },
      "source": [
        "Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-OR6-johB6i"
      },
      "outputs": [],
      "source": [
        "# Check version of packages\n",
        "import torch\n",
        "print(torch)\n",
        "\n",
        "import torchvision\n",
        "print(torchvision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X13Xl_SLhfoq"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "egqoLkXXOfDz"
      },
      "source": [
        "Check Colab GPU and CPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dokutT2vN9VA"
      },
      "outputs": [],
      "source": [
        "# Check GPU info\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl6KnThxOMDp"
      },
      "outputs": [],
      "source": [
        "# Check CPU info\n",
        "!lscpu |grep 'Model name'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wBbmMWruZkkw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOgPs_RObYX4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "if not os.path.exists(\"images.tar.gz\"):\n",
        "  # Download dataset\n",
        "  !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "\n",
        "  # Download truth label\n",
        "  !wget https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
        "  # Unpack the dataset and ground truth\n",
        "  filename_images = \"images.tar.gz\"\n",
        "  !tar -zxvf {filename_images}\n",
        "\n",
        "  filename_annotations = \"annotations.tar.gz\"\n",
        "  !tar -zxvf {filename_annotations}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ge-66OXBwdsg"
      },
      "source": [
        "## CATEGORIZE DATA AFTER ANIMAL BREED (37 IN TOTAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMyrI7FIwhVW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "import random\n",
        "SEED_FOR_DATA_SPLIT = 13948571\n",
        "temp_state_file = \"temp_state_file\"\n",
        "do_img_move = True\n",
        "\n",
        "if os.path.exists(temp_state_file):\n",
        "  with open(temp_state_file, \"r\") as f:\n",
        "    for line in f:\n",
        "      if \"FILES_ALREADY_MOVED\" in line:\n",
        "        do_img_move = False\n",
        "        break\n",
        "\n",
        "if do_img_move:\n",
        "  # 37 classes\n",
        "  annotations_file = \"annotations/list.txt\"\n",
        "  images_base_path = \"images/\"\n",
        "  res_base_dir = \"sorted_imgs\"\n",
        "  classes = defaultdict(list)\n",
        "  with open(annotations_file) as f:\n",
        "      for line in f:\n",
        "          if line.startswith(\"#\"):\n",
        "              continue\n",
        "          name, class_id, species, _ = line.strip().split(\" \")\n",
        "          classes[int(class_id)].append(name + \".jpg\")\n",
        "\n",
        "  for class_id, files in classes.items():\n",
        "      os.makedirs(f\"{res_base_dir}/{str(class_id)}/\", exist_ok=True)\n",
        "      \n",
        "      number_of_files = len(files)\n",
        "      for i in range(number_of_files):\n",
        "          shutil.move(os.path.join(images_base_path, files[i]), f\"{res_base_dir}/{str(class_id)}/{files[i]}\")\n",
        "\n",
        "  with open(temp_state_file, \"a\") as f:\n",
        "    f.write(\"FILES_ALREADY_MOVED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAgl15Oi3jZ5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initial inspiration: https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets \n",
        "def dataset_split(dataset, ratio_to_first_split):\n",
        "  first_indices, second_indices, _, _ = train_test_split(\n",
        "        range(len(dataset)),\n",
        "        dataset.targets,\n",
        "        stratify=dataset.targets,\n",
        "        train_size=ratio_to_first_split,\n",
        "        random_state=SEED_FOR_DATA_SPLIT\n",
        "      )\n",
        "  \n",
        "  return first_indices, second_indices\n",
        "\n",
        "\n",
        "def subset_split(dataset, subset_indices, ratio_to_first_split):\n",
        "  target_labels = list(map(lambda i: dataset.targets[i], subset_indices))\n",
        "  \n",
        "  first_indices, second_indices, _, _ = train_test_split(\n",
        "        subset_indices,\n",
        "        target_labels,\n",
        "        stratify=target_labels,\n",
        "        train_size=ratio_to_first_split,\n",
        "        random_state=SEED_FOR_DATA_SPLIT\n",
        "      )\n",
        "  \n",
        "  return first_indices, second_indices\n",
        "\n",
        "\n",
        "def _get_datasets(ratio_in_labled_train_dataset, transforms_dict):\n",
        "  \"\"\"Return format: test_dataset, val_dataset, labled_train_dataset, unlabled_train_dataset \"\"\"\n",
        "  \n",
        "  labled_train_dataset = datasets.ImageFolder(\"sorted_imgs\", transforms_dict[\"labled\"])\n",
        "  unlabled_train_dataset = datasets.ImageFolder(\"sorted_imgs\", transforms_dict[\"unlabled\"])\n",
        "  test_and_val_dataset = datasets.ImageFolder(\"sorted_imgs\", transforms_dict[\"test_and_val\"])\n",
        "\n",
        "  train_and_val_indices, test_indices = dataset_split(labled_train_dataset, ratio_to_first_split=0.9)\n",
        "  train_indices, val_indices = subset_split(labled_train_dataset, train_and_val_indices, ratio_to_first_split=0.75)\n",
        "  \n",
        "  test_dataset = Subset(test_and_val_dataset, test_indices)\n",
        "  val_dataset = Subset(test_and_val_dataset, val_indices)\n",
        "\n",
        "  if ratio_in_labled_train_dataset == 1.0:\n",
        "    return test_dataset, val_dataset, Subset(labled_train_dataset, train_indices), None\n",
        "\n",
        "  train_labled_indices, train_unlabled_indices = subset_split(labled_train_dataset, train_indices, ratio_to_first_split=ratio_in_labled_train_dataset)\n",
        "  return test_dataset, val_dataset, Subset(labled_train_dataset, train_labled_indices), Subset(unlabled_train_dataset, train_unlabled_indices)\n",
        "\n",
        "\n",
        "def get_dataloaders_dict(ratio_in_labled_train_dataset, transforms_dict, batch_size):\n",
        "  \"\"\" Note: currently test set is fixed at 10 %, val at 20 % and rest (labled + unlabled) is 70 %\"\"\"\n",
        "\n",
        "  test_dataset, val_dataset, labled_train, unlabled_train = _get_datasets(ratio_in_labled_train_dataset, transforms_dict)\n",
        "  dataloaders_dict = {\n",
        "    \"test\": torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
        "    \"val\": torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
        "    \"labled\": torch.utils.data.DataLoader(labled_train, batch_size=batch_size, shuffle=True, num_workers=2),\n",
        "  }\n",
        "  # number of supervised and unsupervised batches will be the same. This will make looping through the dataloader easier\n",
        "  nbr_supervised_batches = len(dataloaders_dict[\"labled\"].batch_sampler)\n",
        "  nbr_unsupervised_samples = len(unlabled_train.indices)\n",
        "  unsupervised_batch_size = round(nbr_unsupervised_samples / nbr_supervised_batches)\n",
        "  dataloaders_dict[\"unlabled\"] = None if not unlabled_train else torch.utils.data.DataLoader(unlabled_train, batch_size=unsupervised_batch_size, shuffle=True, num_workers=2)\n",
        "  \n",
        "  return dataloaders_dict\n",
        "\n",
        "\n",
        "input_size = 224\n",
        "data_transforms = {\n",
        "    'labled': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size,scale=(0.9,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'unlabled': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(input_size,scale=(0.9,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test_and_val': transforms.Compose([\n",
        "        transforms.Resize(input_size),\n",
        "        transforms.CenterCrop(input_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "dataloaders_dict = get_dataloaders_dict(ratio_in_labled_train_dataset=0.9, transforms_dict=data_transforms, batch_size=9)\n",
        "print(dataloaders_dict.keys())\n",
        "# NOTE: there is a new way to get number of batches:\n",
        "# Ex. len(dataloaders_dict[\"test\"].batch_sampler)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nJoDoM_Tph6p"
      },
      "source": [
        "Download model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2aEPMod9w8"
      },
      "source": [
        "### Finetune resnet-18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4c3JPULEWaD"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "target_labels = list(map(lambda i: dataloaders_dict[\"labled\"].dataset.dataset.targets[i], dataloaders_dict[\"labled\"].dataset.indices))\n",
        "print(Counter(target_labels))\n",
        "#len(dataloaders_dict[\"unlabled\"].dataset.indices)\n",
        "print(len(dataloaders_dict[\"labled\"].dataset), len(dataloaders_dict[\"unlabled\"].dataset))\n",
        "print(len(dataloaders_dict[\"labled\"].batch_sampler), len(dataloaders_dict[\"unlabled\"].batch_sampler))\n",
        "dataloaders_dict[\"labled\"].batch_size, dataloaders_dict[\"unlabled\"].batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfyORVvyecRJ"
      },
      "outputs": [],
      "source": [
        "# Top level data directory. Here we assume the format of the directory conforms\n",
        "#   to the ImageFolder structure\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(device)\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(\"cuda:0\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oZa24RqahA7t"
      },
      "source": [
        "Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXD6oA1Se33P"
      },
      "outputs": [],
      "source": [
        "def test_acc(model, dataloader_dict):\n",
        "  model.eval()\n",
        "  running_corrects = 0\n",
        "  for i, (inputs, labels) in enumerate(dataloader_dict[\"test\"]):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    _, pred_for_labled_data = torch.max(outputs, 1)                    \n",
        "    running_corrects += torch.sum(pred_for_labled_data == labels.data)\n",
        "  test_acc = running_corrects.double() / len(dataloader_dict[\"test\"].dataset)\n",
        "  \n",
        "  return test_acc.item()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlPRZkECs1jo"
      },
      "outputs": [],
      "source": [
        "def load_resnet_with_unfrozen_layers(num_unfrozen_layers, unfreeze_bn, num_classes=37):\n",
        "  model_ft = models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "  for name, param in model_ft.named_parameters():\n",
        "    if not any(map(lambda layer_str: layer_str in name, [f\"layer{4-i}\" for i in range(num_unfrozen_layers)])) or (not unfreeze_bn and \"bn\" in name):\n",
        "      param.requires_grad = False\n",
        "\n",
        "  num_ftrs = model_ft.fc.in_features\n",
        "  model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model_ft = model_ft.to(device)\n",
        "\n",
        "  return model_ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_cifar_resnet():\n",
        "    num_classes = 10\n",
        "    model_ft = models.resnet18()\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    return model_ft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2K5isq7D_qL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def alpha_scheduler(current_epoch_num, T1, T2, max_alpha):\n",
        "  # PART 1: [ALPHA = 0]\n",
        "  if (current_epoch_num) < (T1): \n",
        "    return 0\n",
        "\n",
        "  # PART 2: epoch \"T1\" ---> epoch \"T2\"\n",
        "  elif (current_epoch_num) < (T2):\n",
        "    return (current_epoch_num-T1)/(T2-T1) * max_alpha\n",
        "  \n",
        "  # PART 3: [ALPHA = max_alpha]\n",
        "  return max_alpha\n",
        "\n",
        "\n",
        "def train_model_pseudo_labeling(model, dataloaders, criterion, optimizers, schedulers, experiment):\n",
        "    since = time.time()\n",
        "    num_epochs = experiment.get_parameter(\"num_epochs\")\n",
        "    val_acc_history = []\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    step = -1\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        # training phase\n",
        "        phase = \"train\"\n",
        "        model.train()  # Set model to training mode\n",
        "        #alpha = alpha_scheduler(epoch, T1=experiment.get_parameter(\"T1\"), T2=experiment.get_parameter(\"T2\"), max_alpha=experiment.get_parameter(\"max_alpha\")) # Update alpha\n",
        "        #print('ALPHA =', alpha)\n",
        "        alpha = experiment.get_parameter(\"fixed-alpha\")\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Create tqdm progress bar object\n",
        "        print(f\"Processing {phase} data\")\n",
        "\n",
        "        # Iterate over data.\n",
        "        \n",
        "        for (labled_input, labels), (unlabled_input, secret_labels) in zip(dataloaders[\"labled\"], dataloaders[\"unlabled\"]):\n",
        "            step += 1\n",
        "            labled_input = labled_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "            unlabled_input = unlabled_input.to(device)\n",
        "            secret_labels = secret_labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            for optimizer in optimizers:\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            labling_confidence = experiment.get_parameter(\"labling_confidence\")\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # supervised forward pass\n",
        "                supervised_forward_pass_outputs = model(labled_input)\n",
        "                _, pred_for_labled_data = torch.max(supervised_forward_pass_outputs, 1)\n",
        "                supervised_loss = criterion(supervised_forward_pass_outputs, labels)\n",
        "\n",
        "                # pseudo labeled forward pass\n",
        "                unsupervised_out = model(unlabled_input)\n",
        "                softmax_unsupervised_out = nn.functional.softmax(unsupervised_out, dim=1)\n",
        "                max_probs, pseudo_labels = torch.max(softmax_unsupervised_out, dim=1)\n",
        "                # only use pseudolables with high enough confidence\n",
        "                pseudo_label_loss = criterion(unsupervised_out[max_probs >= labling_confidence], pseudo_labels[max_probs >= labling_confidence])\n",
        "                confident_labels = (max_probs >= labling_confidence).sum().item()\n",
        "                with torch.no_grad():\n",
        "                  in_correct_label_count = (pseudo_labels[max_probs >= labling_confidence] != secret_labels[max_probs >= labling_confidence]).sum().item()\n",
        "                  experiment.log_metric(\"confident pseudo labels\", confident_labels, step=step, epoch=epoch)\n",
        "                  experiment.log_metric(\"incorrect pseudo labels ratio\", in_correct_label_count / max(1, confident_labels), step=step, epoch=epoch)\n",
        "                if pseudo_label_loss.isnan().any():\n",
        "                  pseudo_label_loss = 0\n",
        "\n",
        "                # combined loss\n",
        "                loss = supervised_loss + alpha * pseudo_label_loss\n",
        "\n",
        "                # backward + optimize only if in training phase\n",
        "                loss.backward()\n",
        "                for optimizer in optimizers:\n",
        "                  optimizer.step()\n",
        "                for scheduler in schedulers:\n",
        "                  if type(scheduler) == lr_scheduler.OneCycleLR:\n",
        "                    scheduler.step()\n",
        "\n",
        "            experiment.log_metric(\"unsupervised loss\", pseudo_label_loss, step=step, epoch=epoch)\n",
        "            experiment.log_metric(\"scaled unsupervised loss\", alpha * pseudo_label_loss, step=step, epoch=epoch)\n",
        "            experiment.log_metric(\"supervised loss\", supervised_loss, step=step, epoch=epoch)\n",
        "            experiment.log_metric(\"alpha\", alpha, step=step, epoch=epoch)\n",
        "            # statistics\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(pred_for_labled_data == labels.data)\n",
        "\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[\"labled\"].batch_sampler) # note len(dataloaders[phase].dataset) is nbr of samples not nbr of batches\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[\"labled\"].dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        experiment.log_metric(f\"train accuracy\", epoch_acc, step=step, epoch=epoch)\n",
        "\n",
        "        for scheduler in schedulers:\n",
        "          if scheduler is None or scheduler == lr_scheduler.OneCycleLR:\n",
        "            continue\n",
        "          if type(scheduler) == lr_scheduler.ReduceLROnPlateau:\n",
        "            scheduler.step(epoch_acc)\n",
        "          else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # validaiton phase starts here\n",
        "\n",
        "        phase = \"val\"\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Create tqdm progress bar object\n",
        "        print(f\"Processing {phase} data\")\n",
        "\n",
        "        # Iterate over data.\n",
        "        for (labled_input, labels) in dataloaders[\"val\"]:\n",
        "            labled_input = labled_input.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            for optimizer in optimizers:\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            # track history if only in train\n",
        "            with torch.set_grad_enabled(False):\n",
        "                # Get model outputs and calculate loss\n",
        "                  supervised_forward_pass_outputs = model(labled_input)\n",
        "                  _, pred_for_labled_data = torch.max(supervised_forward_pass_outputs, 1)\n",
        "                  loss = criterion(supervised_forward_pass_outputs, labels)\n",
        "                    \n",
        "            # statistics\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(pred_for_labled_data == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloaders[\"val\"].batch_sampler) # note len(dataloaders[phase].dataset) is nbr of samples not nbr of batches\n",
        "        epoch_acc = running_corrects.double() / len(dataloaders[\"val\"].dataset)\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # deep copy the model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        \n",
        "        val_acc_history.append(epoch_acc)\n",
        "        experiment.log_metric(f\"val accuracy\", epoch_acc, step=step, epoch=epoch)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    \n",
        "    experiment.log_metric(\"best model test acc\", test_acc(model, dataloaders_dict))\n",
        "    experiment.end()\n",
        "\n",
        "    return model, val_acc_history"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AcaIAGdqk5oB"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cruuro4uJIo"
      },
      "source": [
        "Cell for experimentation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W4qWxREKlPqR"
      },
      "source": [
        "Run Training and Validation Step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZH1uwnalThf"
      },
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "  \"batch_size\": 9, \n",
        "  \"num_epochs\": 15,\n",
        "  #\"T1\": 6,\n",
        "  #\"T2\": 12,\n",
        "  #\"max_alpha\": 3,\n",
        "  \"fixed-alpha\": 1,\n",
        "  \"labling_confidence\": 0.95,\n",
        "  \"lr\": 0.001,\n",
        "  \"weight_decay\": 0.005,\n",
        "  \"ratio_of_labled_data\": 0.25,\n",
        "  \"seed\": SEED_FOR_DATA_SPLIT,\n",
        "  \"unfrozen_layers\": 0,\n",
        "}\n",
        "\n",
        "runs = [\n",
        "    (\"ratio_of_labled_data\", 0.9),\n",
        "    (\"ratio_of_labled_data\", 0.75),\n",
        "    (\"ratio_of_labled_data\", 0.5),\n",
        "    (\"ratio_of_labled_data\", 0.25),\n",
        "    (\"ratio_of_labled_data\", 0.10),\n",
        "    (\"ratio_of_labled_data\", 0.02),\n",
        "]\n",
        "\n",
        "for run in runs:\n",
        "  hyperparams_copy = hyperparams.copy()\n",
        "  hyperparams_copy[run[0]] = run[1]\n",
        "  experiment = comet_ml.Experiment(\n",
        "    api_key=\"API-KEY-HERE\", \n",
        "    project_name=\"dd2424-project\",\n",
        "    workspace=\"storsorken\",\n",
        "  )\n",
        "  #for experiment in opt.get_experiments():\n",
        "  experiment.add_tag(\"TAG-HERE\")\n",
        "\n",
        "  experiment.log_parameters(hyperparams_copy)\n",
        "\n",
        "  unfrozen_layers = 0\n",
        "  model_ft = model_ft = load_resnet_with_unfrozen_layers(experiment.get_parameter(\"unfrozen_layers\"), True, num_classes=37)\n",
        "\n",
        "\n",
        "  # creates a dict that groups model params into a dict with lists.\n",
        "  # all params for ex. layer4 can be accessed using layer_param_dict[\"layer4\"]\n",
        "\n",
        "  layer_param_dict = defaultdict(list)\n",
        "  for name,param in model_ft.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      layer_param_dict[name.split(\".\")[0]].append(param)\n",
        "\n",
        "  all_params = []\n",
        "  for key, param_list in layer_param_dict.items():\n",
        "    all_params += param_list\n",
        "\n",
        "  print(layer_param_dict.keys())\n",
        "\n",
        "  optimizers = [\n",
        "      optim.Adam(model_ft.parameters(), lr=experiment.get_parameter(\"lr\"), weight_decay=experiment.get_parameter(\"weight_decay\")),\n",
        "      #optim.Adam(layer_param_dict[\"layer4\"], lr=0.001, weight_decay=1e-5),\n",
        "      #optim.Adam(layer_param_dict[\"layer3\"], lr=0.001),\n",
        "  ]\n",
        "  schedulers = [\n",
        "      lr_scheduler.ReduceLROnPlateau(optimizers[0], patience=2),\n",
        "      #lr_scheduler.LambdaLR(optimizers[1], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
        "      #lr_scheduler.LambdaLR(optimizers[2], lr_lambda=lambda epoch: 0.95 ** epoch),\n",
        "  ]\n",
        "\n",
        "  dataloaders_dict = get_dataloaders_dict(experiment.get_parameter(\"ratio_of_labled_data\"), data_transforms, experiment.get_parameter(\"batch_size\"))\n",
        "\n",
        "\n",
        "  # Setup the loss fxn\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Train and evaluate\n",
        "  model_ft, hist = train_model_pseudo_labeling(model_ft, dataloaders_dict, criterion, optimizers, schedulers, experiment)\n",
        "  #model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizers, schedulers, num_epochs=num_epochs)\n",
        "  test_acc_res = test_acc(model_ft, dataloaders_dict)\n",
        "  print(f\"Test acc: {test_acc_res}\")\n",
        "  with open(\"confident_pseudo_labels.txt\", \"a\") as f:\n",
        "    line = f\"ratio_of_labled_data: {str(run[1])}. Test acc: {str(test_acc_res)}. Val: {list(map(lambda x: x.item(), hist))}\\n\"\n",
        "    f.write(line)\n",
        "\n",
        "# plot results\n",
        "plt.plot(range(1, len(hist) + 1), list(map(lambda x: x.item(), hist)))\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Val accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DLW1d2nhQtt"
      },
      "outputs": [],
      "source": [
        "# NOTE: run this line very sparinlgly\n",
        "#print(f\"Test acc: {test_acc(model_ft, dataloaders_dict)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rKIWNKt63HY"
      },
      "outputs": [],
      "source": [
        "#len(dataloaders_dict[\"val\"].batch_sampler), (dataloaders_dict[\"val\"].batch_size), (dataloaders_dict[\"val\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "confident-pseudo-labels.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
